\chapter{Discussion}
\label{ch:discussion}

Convolutional neural networks serve as a powerful tool to solve high dimensional problems. CNNs give good results when, used with euclidean data (images, videos, sounds) that are compositional. Compositional features can be extracted and fed to a classifier, etc. These can be represented using euclidean domain that are regular spatial structures (for Eg. distance of adjacent pixels in an image are the same). To learn from non-euclidean data (real world data) like social networks web, Knowledge Graphs, etc. which are in the form of graphs, CNNs are adapted into Graph Convolutional Neural Networks (GCNs). They are similar to CNNs, but, instead of aggregating individual weights (distance of a node to adjacent nodes), a single shared weight is used by approximation. This solves the limitations of CNNs when nodes have non uniform neighbors. Most DSRE approaches are aimed towards semi-supervised models\cite{smirnova2018relation}. The author Duan et. al.\cite{duan2019hybrid}, aims to show that GCNs are useful for unsupervised DSRE. Limitations of semi-supervised model proposed by Kipf et. al.\cite{kipf2016semi}, have been overcome with this HG model.

\newpar
Proposed HG model is based on models proposed by Daojian Zeng et. al.,\cite{zeng2014relation} and Wenyuan Zeng et. al.,\cite{zeng2016incorporating}. In a novel way, incorporating relation path information in neural RE show a baseline results of efficient DSRE. Common limitation in dealing with non-euclidean data is noise problem. There are standard methods to solve noise problem for euclidean data\cite{zimek2012survey}, but not for the later case. Author Duan et. al.,\cite{duan2019hybrid} uses an attention mechanism by selecting more relevant features to reduce the affect of noise using weighted sum operation over all features. Higher weight is assigned to important features, and can be altered to select or deselect those features that have high noise. Noise is a vague concept. To understand noise, let us consider two features, entity types and relations. For a certain use-case, that has more information on relations, but not much information about entity types. Model is said to have more noise, if entity types feature is given more weight. Prediction based on such training data would be less efficient. This feature of GCNs is used in HG model, which yields higher precision compared previous approaches. Compared to previous approaches, feature information is encoded using various feature encoders and the resultant eigen vectors(simply vectors) are embedded into hybrid graph. This embedding also embeds noisy data during encoding phase, that gets carried on to next step. The only way of controlling the affects of noise is using weight mechanism in attention layer for a hidden layer of GCN. This improves the precision but does not completely eliminate the noise problem which is the suggested future work.

\newpar
One major achievement in HG model is, fusing heterogeneous information from the feature encoders, into a single hybrid graph, as they have different vector embedding. Where each vector represents a node in the graph. An adjacency matrix is used to explain the correlation between nodes. For each instance, and each vector embeddings, this varying structure is converted into a fixed structure using adjacent matrix. Then, high-level features are extracted by using GCN. The training phase for huge corpus takes a very large time. Decent sized dataset was used by aligning Wikidata relations with New York Times Corpus (NYT) for PCCNs\cite{zeng2016incorporating}. Wikidata has more than 80 million triple facts and 20 million entities which is a large sample size to do training. The experimental setup used for PCCNs is used as is, with small changes in parameters like learning rate for SGD, word embedding size, etc. This will not affect the overall results as main goal was comparison. Core advantage of deep learning techniques is, less time for testing phase. Other machine learning techniques which try to learn based on existing data take less training time and more testing time. Author Duan et. al.,\cite{duan2019hybrid} discusses about run-time in less detail as it is prevalent that run-time depends on configuration of machine used, so not applicable in this case. 

\newpar
Author Duan et. al.,\cite{duan2019hybrid} discusses about examples of testing dataset that do not occur. Using long-tail relations, HG model is able to out perform other models and find some score where other models fail. This proves author's assumption that, embedding additional information apart from a specific feature will yield better results for unseen data. 